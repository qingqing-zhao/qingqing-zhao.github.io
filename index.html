<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Helvetica, Avenir, sans-serif;
    font-style: normal;
    font-size: 14px;
    font-weight: 400
  }
  heading {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Helvetica, Avenir, sans-serif;
    font-style: normal;
    font-size: 14px;
    font-weight: 400
  }
  hr
  {
    border: 0;
    height: 0.5px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0.6), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0.6));
  }
  strong {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Helvetica, Avenir, sans-serif;
    font-style: normal;
    font-size: 14px;
    font-weight: 400 
  }
  strongred {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Helvetica, Avenir, sans-serif;
    font-style: normal;
    color: 'red' ;
    font-size: 15px
  }
  sectionheading {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Helvetica, Avenir, sans-serif;
    font-style: normal;
    font-size: 20px;
    font-weight: 400
  }
  pageheading {
    font-family: 'Avenir',Titillium Web,Lato,Verdana, Verdana, Avenir, sans-serif;
    font-style: normal;
    font-size: 28px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/fav_icon.jpg">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Qingqing Zhao</title>
  <meta name="Qingqing Zhao's Homepage" http-equiv="Content-Type" content="Qingqing Zhao's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="76%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Qingqing Zhao (赵青青)</pageheading><br>
  </p>

  <tr>
    <td width="26%" valign="top"><a href="images/Qingqing.jpg"><img src="images/Qingqing.jpg" width="100%" style="border-radius:10px"></a>
    <p align=center>
  | <a href="Qingqing_CV.pdf">CV</a> |
    <a href="mailto:cyanzhao@stanford.edu">Email</a>     | <a href="https://github.com/cyanzhao42">Github</a> | 
    <a href="https://x.com/qingqing_zhao_">X</a> | 
    <a href="https://scholar.google.com/citations?hl=en&user=UVMgJvYAAAAJ&view_op=list_works&sortby=pubdate">Scholar</a> |
    <!-- <a href="https://www.linkedin.com/in/qingqing-zhao-58944b150/">LinkedIn</a> | -->
    </p>
    </td>
    <td width="73%" valign="top" align="justify">
    <p>
      I am a final year Ph.D. student in Electrical Engineering at <a href="https://www.computationalimaging.org/">Stanford Computational Imaging Lab</a> advised by <a href="http://web.stanford.edu/~gordonwz/" >Prof. Gordon Wetzstein</a>.  
    </p>
    <p>
      I am interested in foundation models for perception, control, and modeling.
    </p>
    <p>
      <b style="color:rgb(255, 100, 100);"> 
        I am looking for a research position in industry. Please reach out if you think I could be a good fit!<br><br>
      </b>
      cyanzhao [at] stanford (dot) edu
    </p>

    </p>
    Meet my cat <a href="https://www.instagram.com/bartie_foodie/">Bartie</a>.
    </td>
  </tr>
</table>
<table width="80%" align="center" border="0" cellspacing="0" cellpadding="0">
  <tr><td><sectionheading>&nbsp;&nbsp;Research</sectionheading></td></tr>
</table>
<table width="75%" align="center" border="0" cellspacing="0" cellpadding="15">
</table>

<hr width="75%">


<table width="90%" align="center" border="0" cellspacing="0" cellpadding="10">
  <!-- <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr> -->
</table>
<table width="80%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td width="40%" valign="top" align="center"><a href="https://cot-vla.github.io/">
    <video playsinline autoplay loop muted src="images/cot_vla_video_clip.m4v" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://cot-vla.github.io/" id="COTVLA">
      <heading>CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models</heading></a><br>
      <u>Qingqing Zhao</u>, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Max Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, Tsung-Yi Lin<br>
      <br>
      </p>

      <div class="paper" id="cotvla">
      <a href="https://cot-vla.github.io/">webpage</a> |
      <a href="images/cotvla_paper.pdf">pdf</a> |
      <a href="javascript:toggleblock('cotvla_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('cotvla')" class="togglebib">bibtex</a> |
      <!-- <a href="images/cotvla_paper.pdf">paper</a> | -->
      <a href="images/cot_vla_video.m4v">video</a>

      <p align="justify"> <i id="cotvla_abs">Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks.</i></p>

<pre xml:space="preserve">
@inproceedings{zhao2024cotvla,
  author    = {Qingqing Zhao, Yao Lu, 
    Moo Jin Kim, Zipeng Fu, 
    Zhuoyang Zhang, Yecheng Wu, Max Li, 
    Qianli Ma, Song Han, Chelsea Finn, 
    Ankur Handa, Ming-Yu Liu, Donglai Xiang, 
    Gordon Wetzstein,Tsung-Yi Lin},
    title = {CoT-VLA: 
      Visual Chain-of-Thought Reasoning 
    for Vision-Language-Action Models},
  booktitle = {arXiv},
  year = {2024}
}
</pre>
      </div>
    </td>
  </tr>
  



  <tr>
    <td width="40%" valign="top" align="center"><a href="https://humanoid-ai.github.io">
    <video playsinline autoplay loop muted src="images/humanplus-clip.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://humanoid-ai.github.io" id="HUMANPLUS">
      <heading>HumanPlus: Humanoid Shadowing and Imitation from Humans</heading></a><br>
      Zipeng Fu*, <u>Qingqing Zhao*</u>, Qi Wu*, Gordon Wetzstein, Chelsea Finn<br>
      CoRL 2024<br>
      <b style="color:rgb(255, 100, 100);">Best Paper Award Finalist (top 6)</b>
      </p>

      <div class="paper" id="humanplus">
      <a href="https://humanoid-ai.github.io">webpage</a> |
      <a href="https://humanoid-ai.github.io/HumanPlus.pdf">pdf</a> |
      <a href="javascript:toggleblock('humanplus_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('humanplus')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2406.10454">arXiv</a> |
      <a href="https://github.com/MarkFzp/humanplus">code</a> |
      <a href="https://youtu.be/FPiyv7CIV6I">video</a>

      <p align="justify"> <i id="humanplus_abs">One of the key arguments for building robots that have similar form factors to human beings is that we can leverage the massive human data for training. Yet, doing so has remained challenging in practice due to the complexities in humanoid perception and control, lingering physical gaps between humanoids and humans in morphologies and actuation, and lack of a data pipeline for humanoids to learn autonomous skills from egocentric vision. In this paper, we introduce a full-stack system for humanoids to learn motion and autonomous skills from human data. We first train a low-level policy in simulation via reinforcement learning using existing 40-hour human motion datasets. This policy transfers to the real world and allows humanoid robots to follow human body and hand motion in real time using only a RGB camera, i.e. shadowing. Through shadowing, human operators can teleoperate humanoids to collect whole-body data for learning different tasks in the real world. Using the data collected, we then perform supervised behavior cloning to train skill policies using egocentric vision, allowing humanoids to complete different tasks autonomously by imitating human skills. We demonstrate the system on our customized 33-DoF 180cm humanoid, autonomously completing tasks such as wearing a shoe to stand up and walk, unloading objects from warehouse racks, folding a sweatshirt, rearranging objects, typing, and greeting another robot with 60-100% success rates using up to 40 demonstrations.</i></p>

<pre xml:space="preserve">
@inproceedings{fu2024humanplus,
  author    = {Fu, Zipeng and Zhao, Qingqing 
    and Wu, Qi and Wetzstein, Gordon 
    and Finn, Chelsea},
  title     = {HumanPlus: Humanoid Shadowing 
    and Imitation from Humans},
  booktitle = {arXiv},
  year = {2024}
}
</pre>
      </div>
    </td>
  </tr>
  
  <tr>
    <td width="30%" valign="top" align="center">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem"> -->
    <img src="images/physavatar.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://qingqing-zhao.github.io/PhysAvatar" id="physavatar">
      <heading><b>PhysAvatar</b>: Learning the Physics of Dressed 3D Avatars
        from Visual Observations </heading></a><br>
        Yang Zheng*, <u>Qingqing Zhao*</u>, Guandao Yang, Wang Yifan, Donglai Xiang, Florian Dubost, Dmitry Lagun, Thabo Beeler, Federico Tombari, Leonidas Guibas, Gordon Wetzstein<br>
      ECCV 2024
      </p>


      <div class="paper" id="physavatar">
      <a href="https://qingqing-zhao.github.io/PhysAvatar">webpage</a> |
      <a href="https://arxiv.org/abs/2404.04421">pdf</a> |
      <a href="javascript:toggleblock('physavatar_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('physavatar')" class="togglebib">bibtex</a> |
      <!-- <a href="https://github.com/computational-imaging/GraphPDE">github</a> | -->
      <a href="https://youtu.be/J3YkTtXEbGM?si=Fyz9vgZzspJOKRVq">video</a>

      <p align="justify"> <i id="physavatar_abs">Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligned 4D Gaussian technique for spatio-temporal mesh tracking as well as a physically based inverse renderer to estimate the intrinsic material properties. PhysAvatar integrates a physics simulator to estimate the physical parameters of the garments using gradient-based optimization in a principled manner. These novel capabilities enable PhysAvatar to create high-quality novel-view renderings of avatars dressed in loose-fitting clothes under motions and lighting conditions not seen in the training data. This marks a significant advancement towards modeling photorealistic digital humans using physically based inverse rendering with physics in the loop. </i></p>

<pre xml:space="preserve">
  @article{zheng2024physavatar,
    title={PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations},
    author={Zheng, Yang and Zhao, Qingqing and Yang, Guandao and Yifan, Wang and Xiang, Donglai and Dubost, Florian and Lagun, Dmitry and Beeler, Thabo and Tombari, Federico and Guibas, Leonidas and others},
    journal={arXiv preprint arXiv:2404.04421},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="30%" valign="top" align="center">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem"> -->
    <img src="images/wos_ncv.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://qingqing-zhao.github.io/" id="wos_ncv">
      <heading>Neural Control Variates with Automatic Integration</heading></a><br>
      Zilu Li*, Guandao Yang*, <u>Qingqing Zhao</u>, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzsteinn<br>
      SIGGRAPH 2024
      </p>

      <div class="paper" id="WOS">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem">webpage</a> | -->
      <!-- <a href="https://arxiv.org/pdf/2112.02094.pdf">pdf</a> | -->
      <a href="javascript:toggleblock('wos_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('WOS')" class="togglebib">bibtex</a> |
      <a href="https://github.com/404">github</a> |
      <!-- <a href="https://youtu.be/ov0jxa4xHGU">video</a> -->

      <p align="justify"> <i id="wos_abs">We present a method that uses arbitrary neural network architectures as control variates with automatic differentiation to create unbiased, low-variance, and numerically stable Monte Carlo estimators for various problem setups.
      </i></p>

<pre xml:space="preserve">
  <!-- @article{Zilu2024ncv,
    title={Neural Control Variates with Automatic Integration},
    author={Zilu Li, Guandao Yang, Qingqing Zhao, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzsteinn},
    journal={arXiv preprint arXiv:2310.20249},
    year={2023}
  } -->
</pre>
      </div>
    </td>
  </tr>



  <tr>
    <td width="30%" valign="top" align="center">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem"> -->
    <img src="images/pose2motion.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <!-- <p><a href="https://cyanzhao42.github.io/LearnInverseProblem" id="gpr"> -->
      <heading><b>Pose-to-Motion</b>: Cross-Domain Motion Retargeting with Pose Prior</heading></a><br>
      <u>Qingqing Zhao</u>, Peizhuo Li, Wang Yifan, Olga Sorkine-Hornung, Gordon Wetzstein<br>
      SCA 2024
      </p>

      <div class="paper" id="P2M">
      <a href="https://qingqing-zhao.github.io/pose2motion">webpage</a> |
      <a href="https://arxiv.org/abs/2310.20249">pdf</a> |
      <a href="javascript:toggleblock('p2m_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('P2M')" class="togglebib">bibtex</a> |
      <a href="https://github.com/github-anonymous-submission/pose2motion_submission">github</a> |

      <p align="justify"> <i id="p2m_abs">Creating believable motions for various characters has long been a goal in computer graphics. Current learning-based motion synthesis methods depend on extensive motion datasets, which are often challenging, if not impossible, to obtain. On the other hand, pose data is more accessible, since static posed characters are easier to create and can even be extracted from images using recent advancements in computer vision. In this paper, we utilize this alternative data source and introduce a neural motion synthesis approach through retargeting. Our method generates plausible motions for characters that have only pose data by transferring motion from an existing motion capture dataset of another character, which can have drastically different skeletons. Our experiments show that our method effectively combines the motion features of the source character with the pose features of the target character, and performs robustly with small or noisy pose data sets, ranging from a few artist-created poses to noisy poses estimated directly from images. Additionally, a conducted user study indicated that a majority of participants found our retargeted motion to be more enjoyable to watch, more lifelike in appearance, and exhibiting fewer artifacts.</i></p>

<pre xml:space="preserve">
  @article{zhao2023pose,
    title={Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior},
    author={Zhao, Qingqing and Li, Peizhuo and Yifan, Wang and Sorkine-Hornung, Olga and Wetzstein, Gordon},
    journal={SCA},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="30%" valign="top" align="center">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem"> -->
    <img src="images/gpr.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://github.com/merlresearch/DeepBornFNO" id="gpr">
      <heading>Deep Born Operator Learning for Reflection <br>Tomographic Imaging</heading></a><br>
      <u>Qingqing Zhao</u>, Yanting Ma, Petros T. Boufounos,<br> Saleh Nabi, Hassan Mansour<br>
      ICASSP 2023 
      </p>

      <div class="paper" id="GPR">
      <a href="https://www.merl.com/publications/docs/TR2023-029.pdf">pdf</a> |
      <a href="javascript:toggleblock('GPR_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('GPR')" class="togglebib">bibtex</a> |
      <a href="https://github.com/merlresearch/DeepBornFNO">github</a> |
      <a href="https://youtu.be/RUdxovimitY">video</a>

      <p align="justify"> <i id="GPR_abs">Recent developments in wave-based sensor technologies, such as ground penetrating radar (GPR), provide new opportunities for accurate imaging of underground scenes. Given measurements of the scattered electromagnetic wavefield, the goal is to estimate the spatial distribution of the permittivity of the underground scenes. However, such problems are highly ill-posed, difficult to formulate, and computationally expensive. In this paper, we propose a physics-inspired machine learning-based method to learn the wave-matter interaction under the GPR setting. The learned forward model is combined with a learned signal prior to recover the permittivity distribution of the unknown underground scenes. We test our approach on a dataset of 400 permittivity maps with a three-layer background, which is challenging to solve using existing methods. We demonstrate via numerical simulation that our method achieves a 50% improvement in mean squared error over the benchmark machine learning-based solvers for reconstructing layered underground scenes.</i></p>

<pre xml:space="preserve">
@inproceedings{BornGPR,
    title={Deep Born Operator Learning for Reflection Tomographic Imaging},
    author={Qingqing Zhao*, Yanting Ma, Petros T. Boufounos,
        Saleh Nabi, Hassan Mansour}
    journal={under_review},
    year={2022}
}
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://snap.stanford.edu/lamp/">
      <!-- <a href="https://cyanzhao42.github.io/LearnInverseProblem"> -->
    <img src="images/rlgnn.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <!-- <p><a href="https://cyanzhao42.github.io/LearnInverseProblem" id="rlgnn22"> -->
      <heading>Learning Controllable Adaptive Simulation <br>for Multi-resolution Physics</heading></a><br>
      Tailin Wu*, Takashi Maruyama*, <u>Qingqing Zhao*</u>,<br> Gordon Wetzstein, Jure Leskovec<br>
      ICLR 2023, Spotlight
      </p>

      <div class="paper" id="rlgnn22">
      <a href="https://snap.stanford.edu/lamp/">webpage</a> |
      <a href="https://openreview.net/forum?id=PhktEpJHU3">OpenReview</a> |
      <a href="https://arxiv.org/abs/2305.01122">pdf</a> |
      <a href="javascript:toggleblock('rlgnn22_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('rlgnn22')" class="togglebib">bibtex</a> |
      <a href="https://github.com/snap-stanford/lamp">github</a> |
      <!-- <a href="https://youtu.be/ov0jxa4xHGU">video</a> -->

      <p align="justify"> <i id="rlgnn22_abs">Simulating the time evolution of physical systems is pivotal in many scientific and engineering problems. An open challenge in simulating such systems is their multi-scale dynamics: a small fraction of the system is extremely dynamic, and requires very fine-grained resolution, while a majority of the system is changing slowly and can be modeled by coarser spatial scales. Typical learning-based surrogate models use a uniform spatial scale, which needs to resolve to the finest required scale and can waste a huge compute to achieve required accuracy. In this work, we introduce Learning controllable Adaptive simulation for Multi-scale Physics (LAMP) as the first full deep learning-based surrogate model that jointly learns the evolution model and optimizes appropriate spatial resolutions that devote more compute to the highly dynamic regions. LAMP consists of a Graph Neural Network (GNN) for learning the forward evolution, and a GNN-based actor-critic for learning the policy of spatial refinement and coarsening. We introduce learning techniques that optimizes LAMP with weighted sum of error and computational cost as objective, which allows LAMP to adapt to varying relative importance of error vs. computation tradeoff at inference time. We test our method in a 1D benchmark of nonlinear PDEs and a challenging 2D mesh-based simulation. We demonstrate that our LAMP outperforms state-of-the-art deep learning surrogate models with up to 60.5\% error reduction, and is able to adaptively trade-off computation to improve long-term prediction error.</i></p>

<pre xml:space="preserve">
@inproceedings{Tailingraphpde,
    title={Learning Controllable Adaptive Simulation 
      for Multi-scale Physics},
    author={Tailin Wu, Takashi Maruyama, 
      Qingqing Zhao, Gordon Wetzstein, 
      Jure Leskovec}
    journal={ICLR},
    year={2023}
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://cyanzhao42.github.io/LearnInverseProblem">
      <video playsinline autoplay loop muted src="images/inv_gnn.mov" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://cyanzhao42.github.io/LearnInverseProblem" id="GNNINV22">
      <heading>Learning to Solve PDE-constrained Inverse Problems<br>with Graph Networks</heading></a><br>
      <u>Qingqing Zhao</u>, David B. Lindell, Gordon Wetzstein<br>
      ICML 2022
      </p>

      <div class="paper" id="gnninv22">
      <a href="https://cyanzhao42.github.io/LearnInverseProblem">webpage</a> |
      <!-- <a href="https://arxiv.org/pdf/2112.02094.pdf">pdf</a> | -->
      <a href="javascript:toggleblock('gnninv22_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('gnninv22')" class="togglebib">bibtex</a> |
      <a href="https://github.com/computational-imaging/GraphPDE">github</a> |
      <a href="https://youtu.be/ov0jxa4xHGU">video</a>

      <p align="justify"> <i id="gnninv22_abs">Learned graph neural networks (GNNs) have recently been established as fast and accurate alternatives for principled solvers in simulating the dynamics of physical systems. In many application domains across science and engineering, however, we are not only interested in a forward simulation but also in solving inverse problems with constraints defined by a partial differential equation (PDE). Here we explore GNNs to solve such PDE-constrained inverse problems. Given a sparse set of measurements, we are interested in recovering the initial condition or parameters of the PDE. We demonstrate that GNNs combined with autodecoder-style priors are well-suited for these tasks, achieving more accurate estimates of initial conditions or physical parameters than other learned approaches when applied to the wave equation or Navier-Stokes equations. We also demonstrate computational speedups of up to 90x using GNNs compared to principled solvers.</i></p>

<pre xml:space="preserve">
@inproceedings{qzhao2022graphpde,
    title={Learning to Solve PDE-constrained 
      Inverse Problems 
      with Graph Networks},
    author={Qingqing Zhao and David B. Lindell 
      and Gordon Wetzstein}
    journal={ICML},
    year={2022}
}
</pre>
      </div>
    </td>
  </tr>

 
  <tr>
    <td width="40%" valign="top" align="center">
    <img src="images/modev.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2008.13241 id="MODEV">
      <heading>Minimum Dielectric-Resonator Mode Volumes</heading></a><br>
      <u>Qingqing Zhao</u>, Lang Zhang, Owen D. Miller<br>
      </p>

      <div class="paper" id="modev">
      <a href="https://arxiv.org/abs/2008.13241">pdf</a> |
      <a href="javascript:toggleblock('modev_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('modev')" class="togglebib">bibtex</a> |
      <p align="justify"> <i id="modev_abs">We show that global lower bounds to the mode volume of a dielectric resonator can be computed via Lagrangian duality. State-of-the-art designs rely on sharp tips, but such structures appear to be highly sub-optimal at nanometer-scale feature sizes, and we demonstrate that computational inverse design offers orders-of-magnitude possible improvements. Our bound can be applied for geometries that are simultaneously resonant at multiple frequencies, for high-efficiency nonlinear-optics applications, and we identify the unavoidable penalties that must accompany such multiresonant structures.</i></p>

<pre xml:space="preserve">
@misc{qzhaomodev,
    url = {https://arxiv.org/abs/2008.13241},
    author = {Qingqing Zhao and Lang Zhang and Owen D. Miller},
    title = {Minimum Dielectric-Resonator Mode Volumes},
    publisher = {arXiv},
    year = {2020},
}
</pre>
      </div>
    </td>
  </tr>

 <tr>
  <td width="40%" valign="top" align="center">
  <img src="images/nuclear.png" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
  </a></td>
  <td width="60%" valign="top">
    <p><a href="https://arxiv.org/abs/2008.13241 id="MODEV">
    <heading>Large Isospin Asymmetry in 22Si/22O Mirror <br>Gamow-Teller Transitions 
      Reveals the Halo Structure of 22Al</heading></a><br>
    J. Lee, et. al. (RIBLL Collaboration)<br>
    Physical Review Letters, 2020
    </p>

    <div class="paper" id="modev">
    <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.125.192503">pdf</a> |
    <a href="javascript:toggleblock('nuclear_abs')">abstract</a> |
    <!-- <a shape="rect" href="javascript:togglebib('modev')" class="togglebib">bibtex</a> | -->

    <p align="justify"> <i id="nuclear_abs">β-delayed one-proton emissions of 22Si, the lightest nucleus with an isospin projection Tz ¼ −3, are studied with a silicon array surrounded by high-purity germanium detectors. Properties of β-decay branches and the reduced transition probabilities for the transitions to the low-lying states of 22Al are determined. Compared to the mirror β decay of 22O, the largest value of mirror asymmetry in low-lying states by far, with δ ¼ 209ð96Þ, is found in the transition to the first 1þ excited state. Shell-model calculation with isospin-nonconserving forces, including the T ¼ 1, J ¼ 2, 3 interaction related to the s1=2 orbit that introduces explicitly the isospin-symmetry breaking force and describes the loosely bound nature of the wave functions of the s1=2 orbit, can reproduce the observed data well and consistently explain the observation that a large δ value occurs for the first but not for the second 1þ excited state of 22Al. Our results, while supporting the proton-halo structure in 22Al, might provide another means to identify halo nuclei.</i></p>

      </div>
    </td>
  </tr>
</table>

<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
  <tr><td><br><p align="right">
  Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a>
  </font></p></td></tr>
</table>

</td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('gnninv22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('GPR_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('rlgnn22_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('modev_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('nuclear_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('p2m_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('wos_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('physavatar_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('humanplus_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('cotvla_abs');
</script>
</body>

</html>
